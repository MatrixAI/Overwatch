
## Centralized Logging

[**Centralized logging**](http://jasonwilder.com/blog/2012/01/03/centralized-logging/) so that multiple logs can be aggregated in a central location.
- File replication: No actual aggregation, has delay.
- Syslog (rsyslog or syslog-ng): daemons that allow processes to send log messages to them and the syslog configuration determines how the are stored. Can be setup over network.
- Distributed Log Collectors: Separate into *client logging agents tier*, *collectors tier* and *storage tier*. Each tier is scalable.
- Hosted Logging Services: logging-as-a-service

### [Architectures](http://jasonwilder.com/blog/2013/07/16/centralized-logging-architecture/)
- *Collection*: consider timeliness requirement when doing file replication, *syslog* is defacto collector
- *Transport*: 
- *Storage*: consider log retention duration, log volumn and access methods
- *Analysis*: 
- *Alerting*: for error reporting and monitoring

## [Debugging Production Systems](https://www.infoq.com/presentations/Debugging-Production-Systems?utm_source=infoq&utm_medium=popular_links_homepage)
- Every dump is sacred 
- Fatal failure
    - Postmortem debugging
        - Must have mechanism for saving state on failure
        - Must record sufficient state - must include both program text and program data
        - Must have sufficient data in DRAM, (correctly formed stack, symbol table, type information)
        - sufficent storage
- Transient failure
    - Given a hurricane, find the butterfly
    - DTrace

## Queueing Theory
### Queues
- LIFO, FIFO, Processor sharing (round-robin scheduling for all processes), priority based, shortest first, *Preemptive shortest job first*, shortest remaining first
- Facility
    - Single queue, single server    
    - Single queue, multiple servers
    - Multiple queues each corresponding to a separate server
- Customer behaviour
    - Balking, not join to queue if it is too long
    - Jockeying, switching between queues if it seems to be faster
    - Reneging , leave the queue after waiting too long
    
### [Little's Law](http://web.mit.edu/~sgraves/www/papers/Little%27s%20Law-Published.pdf)
- $L =$ average number of items in the queuing system,
- $W =$ average waiting time in the system for an item, and
- $\lambda =$ average number of items arriving per unit time, the law is
$$L=\lambda W$$

## Dapper (Google Paper) -> OpenTracing project

Tracing infrastructure requires **ubiquity** (monitors all components) and **continuous monitoring** (monitors at all time).

Requirements:
- *Low overhead*: tracing system should have negligible performance impact on running services.
- *Application-level transparency*: application programmers should not need to be aware of the tracing system.
- *Scalability*
- *Availability*: fresher the tracing data, faster the reaction to anomalies.

Solutions:

- Application-level transparency: limit tracing instrumentation ot small corpus of ubiquitous threading?, control flow and RPC library code
- Low overhead and scalability: Adaptive sampling. 

### Distributed tracing 
- *black-box* monitoring: no additional information other than message exchanged. use statistical regression to infer association
    - more portable
    - requires more data to gain sufficent accuracy
- *annotation-based* monitoring: application explicitly tag every message with global identifier.
    - instruments programs
    
#### Dapper traces: trees, spans and annotations
- spans: work unit
    - start and end time, RPC timing data, application-specific annotations 
- trees: relation of work units.
- Timestamp syncing between RPC server/client

#### Instrumentation points
- instrumentation of a few common libraries instead of application code.

#### Trace collection
- Basically log colleciton, as out-of-band data

#### Security and privacy
- Log RPC names and timestamps, does not log payload

### Overhead
#### Trace generation overhead
- allocating unique ids, non-sampled span annotation, sampled span annotation, disk writes

#### Trace collection overhead
- reading local trace data

#### Effects
- large latency degradation and noticible throughput degradation at 1/1 sampling frequency
- at 1/1024 sampling frequency, effects are unnoticible

#### Adaptive sampling
- low sampling frequency is effective for high-throughput online services, but may miss out important events on low workload.

- In development (2010), an adaptive sampling scheme that is parameterized not by a uniform sampling probability, but by a desired rate of sampled traces per unit time

#### Additional sampling during collection
- to reduce total size of data written to central repo

## Consilience

When multiple sources of evidence are in agreement, the conclusion can be very strong even when none of the individual sources of evidence is significantly so on its own.

Because of consilience, the strength of evidence for any particular conclusion is related to how many independent methods are supporting the conclusion, as well as how different these methods are. Those techniques with the fewest (or no) shared characteristics provide the strongest consilience and result in the strongest conclusions. This also means that confidence is usually strongest when considering evidence from different fields, because the techniques are usually very different.

## [Network peformance benchmark](https://stackoverflow.com/questions/889229/how-to-measure-network-performance-how-to-benchmark-network-protocol/958320#958320)

### ptrace
playing with ptrace [32bit-p1](https://www.linuxjournal.com/article/6100), [32bit-p2](https://www.linuxjournal.com/article/6210), 
[64-bit adaptation](http://theantway.com/2013/01/notes-for-playing-with-ptrace-on-64-bits-ubuntu-12-10/)


```c
#include <sys/ptrace.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/reg.h>   /* For constants ORIG_RAX etc */
int main()
{   pid_t child;
    long orig_rax;
    child = fork();
    if(child == 0) {
        ptrace(PTRACE_TRACEME, 0, NULL, NULL);
        execl("/bin/ls", "ls", NULL);
    }
    else {
        wait(NULL);
        orig_rax = ptrace(PTRACE_PEEKUSER,
                          child, 8 * ORIG_RAX,
                          NULL);
        printf("The child made a "
               "system call %ld\n", orig_rax);
        ptrace(PTRACE_CONT, child, NULL, NULL);
    }
    return 0;
}
```

    The child made a system call 59


- Line 12: child make ptrace call `ptrace(PTRACE_TRACEME, 0, NULL, NULL);` so that it can be traced.
- Line 16: parent wait on signal
- Line 13: child reaches execl, sends signal and waits
- Line 17: parent receives signal
- Line 20: parent grabs system call number from `user_regs_struct` and prints
- Line 22: parent lets child continue

```shell
$ grep 59 /usr/include/x86_64-linux-gnu/asm/unistd_64
.h 
#define __NR_execve 59
#define __NR_adjtimex 159
#define __NR_mknodat 259
```

Purpose of [ORIG_EAX](https://stackoverflow.com/questions/6468896/why-is-orig-eax-provided-in-addition-to-eax)
- Have a record of orignal system call number when function returns where EAX itself is overwritten with return addr


```c
#include <sys/ptrace.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <unistd.h>
#include <asm/unistd_64.h>
#include <sys/reg.h>   /* For constants ORIG_RAX etc */

union {
    long num;
    char chars[sizeof(long)];
} wdata;

int main()
{   pid_t child;
    long orig_rax, rax;
    long params[3];
    int status;
    int insyscall = 0;
    child = fork();
    if(child == 0) {
        ptrace(PTRACE_TRACEME, 0, NULL, NULL);
        execl("/bin/ls", "ls", NULL);
    } else {
        while(1) {
            wait(&status);
            if(WIFEXITED(status))
                break;
            orig_rax = ptrace(PTRACE_PEEKUSER,
                              child, 8 * ORIG_RAX, NULL);
            if(orig_rax == __NR_write) {
                if(insyscall == 0) {
                    /* Syscall entry */
                    insyscall = 1;
                    params[0] = ptrace(PTRACE_PEEKUSER,
                                       child, 8 * RBX,
                                       NULL);
                    params[1] = ptrace(PTRACE_PEEKUSER,
                                       child, 8 * RCX,
                                       NULL);
                    params[2] = ptrace(PTRACE_PEEKUSER,
                                       child, 8 * RDX,
                                       NULL);
                    printf("Write called with "
                           "%ld, %ld, %ld\n",
                           params[0], params[1],
                           params[2]);
                }
                else { /* Syscall exit */
                    rax = ptrace(PTRACE_PEEKUSER,
                                 child, 8 * RAX, NULL);
                    printf("Write returned "
                           "with %ld\n", rax);
                    insyscall = 0;
                }
            }
            ptrace(PTRACE_SYSCALL,
                   child, NULL, NULL);
        }
    }
    return 0;
}
```

    Write called with 100, 139952519549268, 100
    2018-08-17.ipynb
    2018-08-18.ipynb
    dapper-2010-1.pdf
    Orientation.ipynb
    Researches.ipynb
    Sketch 2.png
    Write returned with 100


## Time series data storage
### [Characteristics](https://www.xaprb.com/blog/2014/06/08/time-series-database-requirements/)
For writes:

- Write-mostly is the norm; perhaps 95% to 99% of operations are writes, sometimes higher.
- Writes are almost always sequential appends; they almost always arrive in time order. There is a caveat to this.
- Writes to the distant past are rare. Most measurements are written within a few seconds or minutes after being observed, in the worst case.
- Updates are rare.
- Deletes are in bulk, beginning at the start of history and proceeding in contiguous blocks. Deletes of individual measurements or deletes from random locations in history are rare. Efficient bulk deletes are important; as close to zero cost as possible. Non-bulk deletes need not be optimal.
- Due to the above, an immutable storage format is potentially a good thing. As a further consequence of immutable storage, a predefined or fixed schema may be a problem long-term.

For reads, the following usually holds:

- Data is much larger than memory and rarely read, so caching typically doesn’t work well; systems are often IO-bound.
- Reads are typically logically sequential per-series, ascending or descending.
- Concurrent reads and reads of multiple series at once are reasonably common.

### [Catena](https://misfra.me/state-of-the-state-part-iii/)

- Each point is a tuple of {timestamp, value}
- Each point belongs to a metric. e.g. mem.bytes_free
- Each metric belongs to a source. e.g. localhost
- Data is stored in partitions. Each partition is a chunk of time series data with disjoint timestamp ranges. No sharing between partitions

### [RRDtool](http://en.wikipedia.org/wiki/RRDtool)
Stores time series data in a circular buffer based database, new entries may override oldest entries.

## [How NOT to Measure Latency](https://www.youtube.com/watch?v=lJ8ydIuPFeU)

**Latency** The time it took one operation to happen

### The coordinated omission problem
- Measure response time not service time!
- Take care when taking percentile.
