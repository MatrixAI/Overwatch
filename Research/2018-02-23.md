# 23/2/2018

## [Profiling - Wikipedia](https://en.wikipedia.org/wiki/Profiling_%28computer_programming%29)

The section on statistical profilers seemed interesting, though the reference [#8](http://www.cs.utah.edu/dept/old/texinfo/as/gprof.html#SEC12) yields a 404

## [Latency vs Bandwidth - Field Theory](http://www.field-theory.org/articles/latency/)

Provides a good overview of the causes and effects of having more latency than bandwidth or vice versa, providing real world reasoning for the trade-off, and showing the impact on optimal solutions differs where there is more latency or bandwidth.

Note that the solution provided to the high latency scenario (cluster computing) is not actually optimal, but similar to the optimal, which has vertical stripes instead of horizontal.

## [How Not to Measure Latency - Y Combinator](https://news.ycombinator.com/item?id=10485804)

The conversation stemmed from a linked pdf that now yields a 404. I assume [this](https://www.azul.com/files/HowNotToMeasureLatency_LLSummit_NYC_12Nov2013.pdf) is the pdf that was originally linked.

Coordinated Omission happens when measuring a system's latency and the entire system slows down for some time, only the small number of packets processed in that time might be marked as having high latency.

### Latency Requirements

Unlike with a computational task, where we might only care about average computations per second, a networking task often requires that no operation takes more than a certain amount of time: we don't want a website request to take 5 hours. However, a very small number of slightly slow requests is ok. User "pkhuong" phrases this as being concerned with "latency rather than throughput".

Communicating the requirements as percentiles seems appropriate, for example, 50% of requests must be under 20ms, 99.9% under 500ms and 100% under 2s.

### Using Statistics

In essence, a simplistic approach to measuring latency would be to compute statistics such as mean, median, and percentiles on the latency of all packets. Such an approach would lead to statistical bias, emphasising the times when packets are communicated the fastest, since more packets are going through. For monitoring, we are generally more concerned with the opposite, the packets that are transmitted the slowest, so a different model is needed. One possibility is to look at the average latency over time, or at randomly sampled times.

### Suggestions

* Test the measurement system on an artificial scenario, preferably one with random pauses
* Measure the max time and some percentiles

HDR Histogram might be a good tool for simple graphs of latency, with builtin tools to compensate for the issue of Coordinated Omission.

## PACELC

### [CAP Theorem - Wikipedia](https://en.wikipedia.org/wiki/CAP_theorem)

If a system must tolerate a Network Partition (an arbitrary number of packets being dropped), the system can choose to be either consistent or available, it cannot guarantee both. To be consistent, it would wait or reject requests for information that it can't guarantee to be accurate. To be available, it would reply with the most up-to-date information it has, regardless of the fact it cannot guarantee it is accurate.

### [PACELC Theorem - Wikipedia](https://en.wikipedia.org/wiki/PACELC_theorem)

As an extension to the CAP Theorem, a system that is not experiencing a network partition must choose between [low] latency and consistency. To be consistent, a system must check its information is accurate before sending it, introducing latency. The alternative is to send the most up-to-date information without checking it is accurate, reducing latency.

There exist design strategies to balance both Consistency/Availability and Consistency/Latency, and it might be useful to treat the two cases separately, that is to detect network partitions and alter the system's parameters accordingly.

### [Consistency Tradeoffs in Modern Distributed Database System Design](http://cs-www.cs.yale.edu/homes/dna/papers/abadi-pacelc.pdf)

A consistent distributed system looks to the end user as if there is only one node, since all the nodes serve the same information.

The author considers latency above a few seconds to be a lack of availability.

PNUTS has its nodes detect network partitions and swap from favoring low latency normally to consistency during a partition. This is done by having master nodes for each data item, so if a node finds itself disconnected from another node, it will lock all data items that other node is the master of from writing, meaning there is no need for a merge once the network partition ends.

The author pronounces PACELC as "pass-elk"

"once a system is configured to handle inconsistencies, it makes sense to give up consistency for both availability and lower latency"


### [Hazelcast and the Mythical PA/EC System](https://dbmsmusings.blogspot.com.au/2017/10/hazelcast-and-mythical-paec-system.html)

Most systems are either entirely consistent, denoted PC/EC, or always available with low latency, denoted PA/EL. PNUTS (made by Yahoo) is an example of a PC/EL system, it favours consistency during a partition and lower latency otherwise (more research?).

Hazelcast can act as a PA/EC system, by always referring to the primary data source and updating the local copy, then referring to the local copy only when it cannot access the primary data source. Hazelcast can be configured to be PA/EL instead.

"PA/EC systems thus only make sense for applications for which availability takes priority over consistency, but where the code that handles inconsistencies needs to be run as infrequently as possible"

### My Ideas

It might be nice to allow users to specify whether a distributed system will be (PC or PA) and (EC or EL), or somewhere in between. This might require overwatch to detect network partitions.

We might also consider how Matrix as a whole acts during a network partition. If the monitoring and orchestration is centralised, some nodes might lack monitoring during the partition. If it is not, then at the end of the partition we must resolve inconsistencies between the nodes. Further to this, we might consider the balance between consistency and latency of the monitoring/orchestration system itself during normal operation.

## [Distributed monitoring of Peer-to-Peer systems](http://www.lsv.fr/Publis/PAPERS/PDF/AMB-icde08.pdf)

P2PM is a P2P system for monitoring P2P systems. This might be relevant to Matrix, more reading require at a later date.

## [Monitoring Distributed Processes with Intelligent Agents](http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1013&context=csse_fac)

This looks at a system that uses artificial intelligence, and might be worth reviewing at a later date
