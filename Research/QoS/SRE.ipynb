{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Site Reliability Engineering](https://landing.google.com/sre/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Service Level Objectives ([Chapter 4](https://landing.google.com/sre/book/chapters/service-level-objectives.html))\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which behaviors matter for a service and how to measure and evaluate those behaviors are critical to managing a service.\n",
    "\n",
    "**Service level indicators** (SLIs), **objectives** (SLOs), and **agreements** (SLAs) describes basic properties of metrics that matter, what values those metric should be at, and how to react if the expected service can't be provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicators\n",
    "A **quantitative measure** of some aspect of the level of service that is provided, often aggregated into rate, average or percentile.  \n",
    "Some examples are *Request latency*, *Error rate*, *System throughput*.\n",
    "\n",
    "A SLI should ideally directly measure a service level of interest. In reality, a SLI may also measure a proxy of the service level of interest. e.g. server-side latency as a proxy of client-side latency\n",
    "\n",
    "**Availability**, the fraction of the time that a service is usable. Number of \"nines\" notation. \n",
    "\n",
    "#### Possible Indicators\n",
    "- **correctness, availability, latency, throughput, durability,** end-to-end latency\n",
    "\n",
    "#### Collecting Indicators\n",
    "- Both server and client side\n",
    "\n",
    "#### Aggregation\n",
    "- Most metrics are better thought of as **distributions** rather than averages.\n",
    "    - Averages may miss **instantaneous** load and obscure **tail** latency\n",
    "- Using **percentile** helps consider shape of distribution.\n",
    "    - 99th, 99.9th $\\to$ worst case\n",
    "    - 50th (median) $\\to$ typical case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "A **target value** or **range of values** for a service level that is measured by an SLI. e.g. $\\textbf{SLI}\\le\\textbf{target}$, or $\\textbf{lower bound}\\le\\textbf{SLI}\\le\\textbf{upper bound}$\n",
    "\n",
    "Complexities when choosing SLOs:\n",
    "- Some metrics cannot be set a SLO. e.g. queries per second (QPS) is determined by users.  \n",
    "- Other SLIs can have a SLO. e.g. average latency per request  \n",
    "- QPS and latency are related.\n",
    "\n",
    "SLOs helps set user expections about the service.\n",
    "\n",
    "#### Control Measures\n",
    "SLOs can be referenced as to when to take action when a SLI degrades. \n",
    "1. Monitor and measure the system’s SLIs.\n",
    "2. Compare the SLIs to the SLOs, and decide whether or not action is needed.\n",
    "3. If action is needed, figure out what needs to happen in order to meet the target.\n",
    "4. Take that action.  \n",
    "\n",
    "#### Safety Margin\n",
    "Set internal SLOs for responding to SLI degradation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggreements\n",
    "A **contract** with users that includes consequences of meeting (or missing) the SLOs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Distributed Systems ([Chapter 6](https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html))\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Monitoring systems should address symptoms, 'what's broken?', and cause, 'why?'.\n",
    "- Black-box monitoring is symptom-oriented and represents active—not predicted—problems\n",
    "- White-box monitoring inspects the system internals with instrumentation, therefore allows detection of imminent problems, failures masked by retries, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Four Golden Signals\n",
    "- Latency - The time it takes to service a request.\n",
    "    - Separate latency of successful requests and failed requests.\n",
    "- Traffic\n",
    "- Errors\n",
    "- Saturation\n",
    "    - Set utilization target on most constrained resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrumentation and Performance\n",
    "- Do not use mean quantity. Mean values hide imbalanced/skewed details.\n",
    "- For latency, collect request counts bucketed by latencies rather than actual latency values.\n",
    "- For CPU utilization,\n",
    "    1. Record the current CPU utilization each second.\n",
    "    2. Using buckets of 5% granularity, increment the appropriate CPU utilization bucket each second.\n",
    "    3. Aggregate those values every minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Chapters\n",
    "---\n",
    "Contents beyond Chapter 6 should be followed up on and reviewed when specific topic arises in Overwatch.\n",
    "- Chapter 16 - Tracking Outages\n",
    "- Chapter 21 - Handling Overload\n",
    "- Chapter 22 - Addressing Cascading Failures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".c",
   "mimetype": "text/plain",
   "name": "c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
