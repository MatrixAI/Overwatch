# 27/2/2018

## [Quasar](http://www.industry-academia.org/download/2014-asplos-quasar-Stanford-paper.pdf)

Quasar does not rely on resource reservations, since clients will often over-estimate and occasionally under-estimate.

EC2 = (Amazon) Elastic Compute Cloud

"...a workload may be able to use lowend
CPUs if the memory allocation is high or vice versa."

"...interference between co-located workloads
that can lead to severe performance losses."

Scale out (horizontal) = Add more nodes

Scale up (vertical) = Add more resources to a node

Stateful/stateless = Whether a protocol relies on previous knowledge of communications. For most distributed systems, a stateless protocol would be preferred. A stateful protocol would require that users consistently contact the same node.

Quasar can use a small amount of profiling data about a workload and relate it to previous workloads to estimate (categorise) the needs of that workload. In particular, it notes how well the workload reacts to horizontal and vertical scaling, particular configurations, and how it will interfere with other workloads.

Quasar monitors performance of a workload, so if it is not meeting its QoS constraints, it is recategorised to either meet the constraints, or lower resource usage (so if it needs more CPU, it might be moved to a machine with better RAM to lower CPU use).

Server heterogenity refers to how the workload reacts to different types of servers.

Some workloads (Hadoop) have automated methods of defining their QoS constraints. Some workloads (Hadoop) have configuration settings that can be altered to try and improve performance on particular machines.

"...an incoming workload and dataset is profiled on a few servers for a short period of time (a few seconds up to a few minutes..."

" Quasar adjusts the allocation and assignment if possible, or reclassifies and reschedules the workload from scratch"

The complicated math in Section 3.2 might actually be an efficient way of categorising workloads; finding similar workloads with very little information about the current one.

"...classification becomes fast enough that it can be applied on every workload submission, even if the same workload is submitted multiple times with different datasets."

The paper often catgeorises workloads into latency critical, Hadoop-like (computational), and a few other categories, probably since these tasks have very different QoS constraints. There are many ways similar ideas could be encoded into architect, such as having the ability to save and re-use settings, and having presets like "computational" and "low latency". However, these classifications even extend into the way the workloads are profiled by Quasar.

Quasar's information about how well workloads react to scaling, configuration and interference started by doing a lot of profiling on their cluster, which would only be repeated if major changes were amde tot he cluster. Such profiling might need to be automated in matrix if new hardware is detected.

Quasar's greedy scheduler will use high quality resources first. "A potential source of inefficiency is that the scheduler allocates resources on a per-application basis in the order workloads arrive."

Quasar will scale up before scaling out. This is not necessarily optimal, since scaling out can decrease downtime and latency, factors Quasar doesn't seem to account for. However, since using all the resources on a machine doesn't cost much more than only using some, this approach does make sense. Matrix should definitely keep the amount of machines running to a minimum where possible, fully utilising a small number of machines.

"the scheduler employs admission control to prevent oversubscription when no resources are available." How should matrix handle this?

"...Quasar’s overheads are quite low even for short-running applications (batch, analytics) or long running online services."

"Quasar maintains per-workload and per-server state. Perworkload state includes the classification output. For a cluster with 10 server types and 10 sources of interference, we need roughly 256 bytes per workload. The per-server state includes information on scheduled applications and their cumulative resource interference, roughly 128B in total."

"...a single server was sufficient to handle the total state and computation of cluster management."

Quasar's implementation requires framework specific code, which matrix will hopefully lack.

Quasar will notice if a workload is not fulfilling constraints, which could be due to a different phase of computation, increased user traffic, or earlier miscategorisation, and will adjust the resource allocation.

One method Quasar uses to detect phase changes is to run, on the same machine as an actual workload, benchmarks that might interfere with the relevant workload, and uses the result for interference classification. If there is a major change, the workload is flagged as having changed phase.

"...Quasar does not employ load prediction for user-facing services [29, 46]. In future work, we will use such predictors as an additional signal to trigger adjustments for user-facing workloads." This might be worth looking into for matrix.

Section 4.2 and everything it references might be useful to work out the details of how to test and profile workloads.

Some workloads (Hadoop) have features that allow Quasar to flag particular jobs as stragglers, forcing them to be relaunched on different servers.

Note that Quasar appears to have been in development since 2015 or earlier, and does not appear to have been fully released.

How does Quasar scale up?

The main QoS constraints the paper mentions are those based on performance (computations over time?) and latency.

Might be worth looking at references 20, 27, 41.

### From Roger's Email Correspondence

"You could at this point include additional attributes on which option should be selected based on placement constraints for security or network topology reasons."

## [Paragon](http://csl.stanford.edu/~christos/publications/2013.paragon.asplos.pdf)

"...modern servers are not energy-proportional and consume
a large fraction of peak power even at low utilization."

"...since classification is based on robust analytical methods and not merely empirical observation, we have strong guarantees on its accuracy and strict bounds on its overheads."

"...instead of discovering similarities in users’ movie preferences, it finds similarities in applications’ preferences with respect to heterogeneity and interference."

The algorithm for categorising appears to be treating workloads as vectors and trying to find basis vectors.

Heterogeneous multi-core systems exist, maybe that is what this article and the Quasar one meant by heterogeneity?

The [Paragon website](https://paragondc.stanford.edu/paragon) indicates paragon's code will be made publicly available, but it's been 5 years, so probably not.

## [Scaling](https://stackoverflow.com/questions/42034688/aws-scale-out-scale-up/42034927#42034927)

"...in order to scale-up/scale-down you have to restart the instance..."

## [QoS-Aware Admission Control](https://www.usenix.org/system/files/conference/icac13/icac13_delimitrou.pdf)

"[DataCentre] users are interested in two performance metrics; how fast the application starts running (waiting time) and how fast it completes thereafter (execution time)."

This system (ARQ) will, if insufficient resources are available, queue workloads (based on their QoS constraints and needs), and detect when they get closer to violating their constraints and move them to a different (higher priority) queue.

"Resource quality reflects the additional load a server can support without violating application QoS..." This seems to indicate they are measuring resource use on servers to decide if there is room for more application on that server.

"[Resource Quality] is a function of the interference the server can tolerate from the new application, and the interference the new workload can tolerate from applications already running on the machine."

On further reading, it seems they are measuring (using Paragon) how much a pair of processes will interfere (by competing for resources). Using this, they can decide if a process demands higher or lower quality resources, by averaging how much it interferes with each other process. The quality of resources a machine has must be determined by the processes running on that machine.

Reference 27 might be worth a read

## Thoughts

It might be useful for overwatch to know something about the physical location of its nodes, since nodes in the same facility are more likely to fail at the same time, and may interfere with each other when competing over limited bandwidth.

It is hence worth noting that simple calculations on availability or downtime may not be accurate, since nodes in the same location are more likely to fail together, and instances of the same software are more likely to fail together as they may receive similar requests or both take a large amount of traffic at the same time.

While having users specify QoS constraints instead of reservations will limit excessive reservations, it might not prevent users from making the constraints too strict, resulting in a low priority task running on the best equipment, or even throttling of high priority tasks, to meet overly strict constraints. If matrix is being used by other organisations, this could be solved by a good user interface, which allows admins to notice excessive constraints. If matrix is one network that pays for nodes and is paid by workloads, the issue is more complicated.

How would a profiling system like Quasar's adapt to having servers come and go? If matrix were one large network, we might want to consider this. Otherwise, it might be simpler to restart the profiling system when more hardware is added or removed.

### Monitoring

* Latency (processing time of requests) is a common QoS constraint, is easily measured, and can be effected Emergence (by making more instances, or moving them around). Make sure to avoid statistical mistakes when measuring though.
* Downtime seems like a good QoS constraint, is easily measured, could be predicted on possible configurations, and can be effected by Emergence
* CPU, Memory and Storage usage are limited on each node, can be easily measured, and can be effected by Emergence

Worth reading more into common forms of QoS constraints.
