# 25/2/2018

## [System Reliability and Availability](http://www.eventhelix.com/RealtimeMantra/FaultHandling/system_reliability_availability.htm) continued from yesterday

For systems where n things are running and if m fail the system as a whole fails, there is a formula for calculating the availability of the whole system, given the availability of each part. This is more complicated and is not likely to be relevant to Matrix.

## Predicting reliability/availability for new software

[Event Helix](http://www.eventhelix.com/RealtimeMantra/FaultHandling/reliability_availability_basics.htm) suggests that one could estimate time between failures by first estimating defect density.

Using factors such as the process used to develop the code, the complexity and size of the software, the experience of the team designing the software, the amount of code that is re-used from stable projects, and the amount of testing, one can estimate the defect density of software. It might be possible for Overwatch to collect these statistics.

Once we have defect density, we multiply that be number of operations executed per second to get an estimate of failures per second.

Then, by estimating the repair time in case of a failure, which would be seconds if the software is simply restarted, minutes if the node is rebooted, and hours or days if human intervention is needed, we can estimate the downtime of a system: (time to repair)/(time between failures + time to repair).

Hopefully I can find another source of information on this topic.

## Some ideas about availability

Based on Roger's notes on the subject.

Availability can be a complex thing to measure. For soem systems, while one component has failed it might be considered to have 0% availability, but for others it might be only when 100% of components have failed that it has 0% availability, and some systems might correlate availability entirely with number of failed components.

We could consider the avilability of an entire system to be some configurable function of the availability of its components, though it would be convenient to limit the avilable functions.

By using partial availability of a system as a metric, we could consider latency as a factor in availability, for example a system where requests take >200ms to process is 50% available, and where requests take >2s to process is 0% available.

This may be overcomplicating things though, it might be better to define availability so it does not account for latency, or maybe simply accounts for unacceptable latency.

Another idea is that the user sets goals for latency, such as 50% of requests take at most 20ms, 90% take at most 200ms and 100% take at most 2s, and for every request that exceeds that, the system was considered unavailable, so if only 40% of requests took at most 20ms, the system would be considered 10% unavailable.

The problem with this idea is the availability of a system now depends on its QoS constraints, and running systems in sequence or parallel no longer results in simple multiplication of their downtime or availability. However, since multiplying these statistics will only be used to estimate the availability of a system, maybe this isn't a big problem.

## [Everything you know about latency is wrong](https://bravenewgeek.com/everything-you-know-about-latency-is-wrong/)

Since requesting a webpage results in so many requests (hundreds!), it actually becomes very likely that one of those requests is in the top 1% slowest requests. That means most users will experience the 1%.

## World of Events Paper - Notes

a < b means a is before b, and a || b means neither a < b, b < a nor a = b. Basically, exactly one of the following is true: a < b, a > b, a = b, a || b. Extending this, one of the following is true: a <> b, a = b, a || b. < is transitive.

W |= a < b or W |= a || b just means that the expression is true in W. |=\* generalises this to include <>. The distinction exists since W |= a r b is true for one relation for every a != b, while W |=\* a r b is true for one or two relations for a != b. < and || are accurate, <> is inaccurate.

A microcosm is a finite (since cardinality is less than aleph null) subset of events with a strict ordering (I), ie no events in I are ||, and a finite subset of relations that are true (E), and where the two events in the relation are not both in I.

M |-o a r b symbolises that a microcosm M can deduce a r b in some way. To start, if a r b is in I or E, then M |-o a r b. There is also transitivity, if M |-o a < b and M |-o b < c then M |-o a < c. Symmetry, M |-o a || b implies M |-o b || a and M |-o a <> b implies M |-o b <> a.

If M cannot deduce any relation from a to b, that is for no r can we write M |-o a r b, we can write M + a r b for the microcosm with the information a r b, either internal or external. If M |-o a <> b, we write M[a < b] for the microcosm with a < b replacing a <> b. These exist if they do not violate the previous axioms. But if they could be added externally, they never violate the axioms, unless my initial understanding of the axioms was true, in which case Example 2.4 is wrong.

I refuse to accept this paper as the mathematically rigorous paper it tries to present itself as, and instead I will read it purely for ideas.

The ideas presented in Figure 3 might have made more sense as the definition of a microcosm than the definition provided earlier.
